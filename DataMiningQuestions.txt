Different communities have different visions of data mining. Can you give the epistemological perspective on data mining, the one from the database community, the machine learning community, the inductive database vision, the way businesses consider data mining?


Epistemological perspective on data mining:
From an epistemological perspective, data mining is seen as a process of discovering knowledge from data. This involves using statistical and computational techniques to extract patterns and relationships from large datasets. The goal is to uncover insights that may not be immediately apparent and use them to make better decisions or predictions.

Database community:
From the database community perspective, data mining is seen as a natural extension of traditional database systems. The focus is on the efficient management of large datasets and the development of algorithms that can extract valuable information from them. The emphasis is on creating robust and scalable systems that can handle the ever-increasing amounts of data generated by modern applications.

Machine learning community:
From the machine learning community perspective, data mining is viewed as a subset of the broader field of artificial intelligence. The focus is on developing algorithms that can automatically learn from data and make predictions or decisions based on that learning. This involves using statistical and computational techniques such as neural networks, decision trees, and support vector machines to model and analyze data.

Inductive database vision:
The inductive database vision sees data mining as a way to integrate data mining techniques into the traditional database framework. The focus is on developing database systems that can automatically generate models and patterns from data. This involves using techniques such as data summarization, data compression, and data clustering to create a more efficient and effective database system.

Businesses:
For businesses, data mining is seen as a way to extract valuable insights from large amounts of data. The focus is on using data mining techniques to identify trends, patterns, and relationships that can help businesses make more informed decisions. This involves analyzing customer behavior, identifying market trends, and predicting future sales to gain a competitive advantage in the marketplace.










Give an abstract vision of the knowledge discovery process and qualify it.

The knowledge discovery process is a systematic approach to uncovering valuable insights and knowledge from data. It involves a series of steps, including data collection, cleaning, preprocessing, and analysis, with the goal of discovering hidden patterns, relationships, and trends.

The process typically starts with defining the problem or research question and identifying the relevant data sources. Data is then collected and preprocessed, which may involve cleaning, transforming, and reducing the data to a manageable size.

Next, exploratory data analysis is performed to gain an understanding of the data and identify any patterns or anomalies. This is followed by feature selection, where the most relevant features are chosen for the analysis.

Finally, a machine learning model is trained and tested on the data to make predictions or classifications based on the identified patterns and relationships. The results are then interpreted and used to generate actionable insights and knowledge.

Overall, the knowledge discovery process is a powerful tool for extracting valuable information from large datasets. It requires a deep understanding of data preprocessing, statistical analysis, and machine learning techniques, as well as domain expertise to properly interpret the results. The process is iterative and requires careful consideration of the data and the research question to ensure meaningful and useful insights are gained.










When mining data, the analyst should always be careful of the many problems encountered in real datasets. Can you give some of them? 

There are several problems that analysts may encounter when mining data, including:

Missing or incomplete data: Data may be missing or incomplete, either because it was not collected or because it was lost or corrupted. This can lead to biased or inaccurate results if not handled properly.

Outliers: Outliers are data points that are significantly different from the rest of the data. They can skew statistical measures and lead to incorrect conclusions.

Sampling bias: Sampling bias occurs when the sample used in the analysis is not representative of the population. This can lead to incorrect or biased results.

Data snooping: Data snooping occurs when the analyst examines the data in multiple ways to find patterns or relationships, which can lead to false discoveries.

Overfitting: Overfitting occurs when a model is trained on the data so well that it performs poorly on new, unseen data.


















What statistical assumptions are usually made on the objects in the dataset? 

Statistical assumptions that are usually made on the objects in the dataset include normality, independence, and equal variance. Normality assumes that the data is normally distributed, independence assumes that the observations are not dependent on each other, and equal variance assumes that the variance of the data is the same across all observations.

Which one sampling bias violates? 

Sampling bias violates the assumption of representativeness. If the sample is not representative of the population, then the results of the analysis may not generalize well to the population.


What is Berkson's paradox? Give an example.

Berkson's paradox is a statistical phenomenon where a non-random sample of data appears to be correlated due to a selection bias, even though no correlation exists in the population. For example, in a hospital, patients with two diseases may be more likely to be admitted than patients with only one disease. Therefore, if one were to only look at patients admitted to the hospital, it may appear that the two diseases are correlated, even though they are not.




















Many datasets describe objects with attributes. Categorize those attributes w.r.t. their "types" and explain what kind of processes (derivation of other attributes, statistics, and data mining algorithms) requires what "type" of attribute(s).


Attributes in datasets can be categorized into different types based on their characteristics. The most common attribute types are:

Nominal Attributes: Nominal attributes are categorical variables with no order or hierarchy. They represent different categories or labels that objects can belong to, but there is no inherent order to those categories. Examples of nominal attributes include colors, brands, or categories. Nominal attributes are typically used for classification tasks and require algorithms such as decision trees, naive Bayes, or k-nearest neighbors.

Ordinal Attributes: Ordinal attributes are categorical variables with a predefined order or hierarchy. They represent categories or labels that can be ordered based on their magnitude or degree. Examples of ordinal attributes include ratings, sizes, or rankings. Ordinal attributes are typically used for ranking or ordinal regression tasks and require algorithms such as ordinal regression or rank correlation.

Interval Attributes: Interval attributes are numerical variables with a fixed scale and units of measurement. They represent quantities that have a clear zero point but do not have a meaningful ratio between values. Examples of interval attributes include temperature, time, or age. Interval attributes are typically used for statistical analyses and require algorithms such as linear regression, ANOVA, or t-tests.

Ratio Attributes: Ratio attributes are numerical variables with a fixed scale and units of measurement, and a meaningful zero point. They represent quantities that have a clear zero point and a meaningful ratio between values. Examples of ratio attributes include weight, height, or income. Ratio attributes are typically used for statistical analyses and require algorithms such as logistic regression or linear regression.

Processes such as derivation of other attributes, statistics, and data mining algorithms require different types of attributes depending on the specific task at hand. For example, in a classification task, nominal attributes are typically used, while in a regression task, interval or ratio attributes are preferred. Similarly, statistical analyses such as ANOVA or t-tests require interval attributes, while logistic or linear regression requires ratio attributes.

In data mining, feature engineering often involves deriving new attributes from existing ones, and the choice of attribute types used in this process depends on the specific task. For example, a derived attribute representing the ratio of income to debt requires ratio attributes, while a derived attribute representing the difference between two ordinal attributes requires ordinal attributes.













What does it mean for a statistic to be robust? Can you give examples of a robust and a non-robust statistic of the centrality of a distribution? Of its dispersion? Of the correlation between two interval-scaled attributes?



A statistic is considered robust if it is not strongly affected by outliers or extreme values in the data. In other words, the statistic remains relatively stable and accurate even when the data contains some unusual observations. A non-robust statistic, on the other hand, is strongly influenced by outliers and can lead to misleading or incorrect results.

Examples of robust and non-robust statistics for centrality of a distribution:

The median is a robust statistic for centrality because it is not affected by outliers, as it only depends on the position of the middle value. For example, the median of {1, 2, 3, 100} is still 2.5, while the mean is significantly affected and becomes 26.5.
The mean is a non-robust statistic for centrality because it is sensitive to outliers, as it takes into account the magnitude of all values. For example, the mean of {1, 2, 3, 100} is significantly affected and becomes 26.5, while the median remains 2.5.
Examples of robust and non-robust statistics for dispersion of a distribution:

The interquartile range (IQR) is a robust statistic for dispersion because it is not affected by outliers, as it only depends on the spread of the middle 50% of the values. For example, the IQR of {1, 2, 3, 100} is 2, while the standard deviation is significantly affected and becomes 46.38.
The standard deviation is a non-robust statistic for dispersion because it is sensitive to outliers, as it takes into account the magnitude of all values. For example, the standard deviation of {1, 2, 3, 100} is significantly affected and becomes 46.38, while the IQR remains 2.



Examples of robust and non-robust statistics for correlation between two interval-scaled attributes:

Spearman's rank correlation coefficient is a robust statistic for correlation because it is based on the ranks of the values rather than their magnitudes. It measures the degree to which the ranks of two variables are related, rather than the degree to which their values are related. For example, if two variables have a strong monotonic relationship, but there are some outliers, Spearman's rank correlation coefficient will still be high.
Pearson's correlation coefficient is a non-robust statistic for correlation because it is based on the actual values of the variables, and it is sensitive to outliers. It measures the degree to which two variables have a linear relationship, and it can be influenced by the magnitude of the values. For example, if two variables have a strong linear relationship, but there are some outliers, Pearson's correlation coefficient may be lower than expected.



What is an outlier? What visualization helps in discovering outliers w.r.t. one attribute?, two attributes? Do you know a way to detect outliers in a higher-dimensional space?

Mahalanobis Distance: This method uses the Mahalanobis distance, which is a measure of the distance between a point and the center of a distribution, taking into account the covariance matrix of the data. Outliers are defined as points that are far away from the center of the distribution, and their distance is compared to a threshold value based on a probability distribution.




