Different communities have different visions of data mining. Can you give the epistemological perspective on data mining, the one from the database community, the machine learning community, the inductive database vision, the way businesses consider data mining?


Epistemological perspective on data mining:
From an epistemological perspective, data mining is seen as a process of discovering knowledge from data. This involves using statistical and computational techniques to extract patterns and relationships from large datasets. The goal is to uncover insights that may not be immediately apparent and use them to make better decisions or predictions.

Database community:
From the database community perspective, data mining is seen as a natural extension of traditional database systems. The focus is on the efficient management of large datasets and the development of algorithms that can extract valuable information from them. The emphasis is on creating robust and scalable systems that can handle the ever-increasing amounts of data generated by modern applications.

Machine learning community:
From the machine learning community perspective, data mining is viewed as a subset of the broader field of artificial intelligence. The focus is on developing algorithms that can automatically learn from data and make predictions or decisions based on that learning. This involves using statistical and computational techniques such as neural networks, decision trees, and support vector machines to model and analyze data.

Inductive database vision:
The inductive database vision sees data mining as a way to integrate data mining techniques into the traditional database framework. The focus is on developing database systems that can automatically generate models and patterns from data. This involves using techniques such as data summarization, data compression, and data clustering to create a more efficient and effective database system.

Businesses:
For businesses, data mining is seen as a way to extract valuable insights from large amounts of data. The focus is on using data mining techniques to identify trends, patterns, and relationships that can help businesses make more informed decisions. This involves analyzing customer behavior, identifying market trends, and predicting future sales to gain a competitive advantage in the marketplace.





What is regression?, correlation? Which one is closer to supervised classification?, to frequent itemset mining? Can you give reasons why "correlation does not imply causation"?

Regression and correlation are both statistical techniques used to analyze the relationship between two variables.

Regression is a method of modeling the relationship between a dependent variable and one or more independent variables. It is a supervised learning technique that aims to find the best fitting line or curve that describes the relationship between the variables, in order to make predictions or estimate the value of the dependent variable based on the independent variables.

Correlation, on the other hand, measures the strength and direction of the linear relationship between two variables. It is a measure of association that ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation. Correlation is a descriptive technique that can be used to identify patterns or trends in the data, but it does not involve making predictions or estimating values.

Regression is closer to supervised classification because both involve modeling the relationship between variables and making predictions. In supervised classification, the goal is to predict the class label of an instance based on its attributes, while in regression the goal is to predict the value of a dependent variable based on the values of independent variables.

Correlation is closer to frequent itemset mining because both involve identifying patterns or associations in the data. Frequent itemset mining is a technique used in data mining to identify sets of items that frequently occur together in a dataset, while correlation is a measure of the strength and direction of the relationship between two variables.

"Correlation does not imply causation" is a commonly used phrase in statistics and data analysis. It means that just because two variables are correlated, it does not necessarily mean that one causes the other. There may be other factors or variables that influence both variables, or the relationship may be coincidental or spurious. For example, ice cream sales and crime rates may be positively correlated, but it would be incorrect to conclude that eating ice cream causes crime. In this case, the relationship is likely to be spurious, as both variables are influenced by temperature.

What are the assumptions behind the use of the Pearson correlation? Is that measure robust? Can you name a correlation measure that applies to ordinal attributes?

The Pearson correlation coefficient is a measure of the linear relationship between two interval-scaled variables. There are several assumptions behind its use:

Normality: The variables should follow a normal distribution.

Linearity: The relationship between the variables should be linear.

Homoscedasticity: The variance of the dependent variable should be constant across different values of the independent variable.

Independence: The observations should be independent of each other.

Outliers: There should be no outliers in the data.

The Pearson correlation coefficient is not robust, as it is sensitive to outliers and non-normality. It is affected by extreme values and can be biased if the data do not follow a normal distribution.

A correlation measure that applies to ordinal attributes is the Spearman rank correlation coefficient. It is a nonparametric measure of the strength and direction of the monotonic relationship between two variables. It does not assume a linear relationship between the variables, and it is robust to outliers and non-normality. It is calculated by ranking the values of each variable and calculating the Pearson correlation coefficient between the ranks.

What is Simpson's paradox? Can you give an example? Bonus point to those who give a real-world example not in the slides (easy to find on the Web).





Give an abstract vision of the knowledge discovery process and qualify it.

The knowledge discovery process is a systematic approach to uncovering valuable insights and knowledge from data. It involves a series of steps, including data collection, cleaning, preprocessing, and analysis, with the goal of discovering hidden patterns, relationships, and trends.

The process typically starts with defining the problem or research question and identifying the relevant data sources. Data is then collected and preprocessed, which may involve cleaning, transforming, and reducing the data to a manageable size.

Next, exploratory data analysis is performed to gain an understanding of the data and identify any patterns or anomalies. This is followed by feature selection, where the most relevant features are chosen for the analysis.

Finally, a machine learning model is trained and tested on the data to make predictions or classifications based on the identified patterns and relationships. The results are then interpreted and used to generate actionable insights and knowledge.

Overall, the knowledge discovery process is a powerful tool for extracting valuable information from large datasets. It requires a deep understanding of data preprocessing, statistical analysis, and machine learning techniques, as well as domain expertise to properly interpret the results. The process is iterative and requires careful consideration of the data and the research question to ensure meaningful and useful insights are gained.










When mining data, the analyst should always be careful of the many problems encountered in real datasets. Can you give some of them? 

There are several problems that analysts may encounter when mining data, including:

Missing or incomplete data: Data may be missing or incomplete, either because it was not collected or because it was lost or corrupted. This can lead to biased or inaccurate results if not handled properly.

Outliers: Outliers are data points that are significantly different from the rest of the data. They can skew statistical measures and lead to incorrect conclusions.

Sampling bias: Sampling bias occurs when the sample used in the analysis is not representative of the population. This can lead to incorrect or biased results.

Data snooping: Data snooping occurs when the analyst examines the data in multiple ways to find patterns or relationships, which can lead to false discoveries.

Overfitting: Overfitting occurs when a model is trained on the data so well that it performs poorly on new, unseen data.


















What statistical assumptions are usually made on the objects in the dataset? 

Statistical assumptions that are usually made on the objects in the dataset include normality, independence, and equal variance. Normality assumes that the data is normally distributed, independence assumes that the observations are not dependent on each other, and equal variance assumes that the variance of the data is the same across all observations.

Which one sampling bias violates? 

Sampling bias violates the assumption of representativeness. If the sample is not representative of the population, then the results of the analysis may not generalize well to the population.


What is Berkson's paradox? Give an example.

Berkson's paradox is a statistical phenomenon where a non-random sample of data appears to be correlated due to a selection bias, even though no correlation exists in the population. For example, in a hospital, patients with two diseases may be more likely to be admitted than patients with only one disease. Therefore, if one were to only look at patients admitted to the hospital, it may appear that the two diseases are correlated, even though they are not.




















Many datasets describe objects with attributes. Categorize those attributes w.r.t. their "types" and explain what kind of processes (derivation of other attributes, statistics, and data mining algorithms) requires what "type" of attribute(s).


Attributes in datasets can be categorized into different types based on their characteristics. The most common attribute types are:

Nominal Attributes: Nominal attributes are categorical variables with no order or hierarchy. They represent different categories or labels that objects can belong to, but there is no inherent order to those categories. Examples of nominal attributes include colors, brands, or categories. Nominal attributes are typically used for classification tasks and require algorithms such as decision trees, naive Bayes, or k-nearest neighbors.

Ordinal Attributes: Ordinal attributes are categorical variables with a predefined order or hierarchy. They represent categories or labels that can be ordered based on their magnitude or degree. Examples of ordinal attributes include ratings, sizes, or rankings. Ordinal attributes are typically used for ranking or ordinal regression tasks and require algorithms such as ordinal regression or rank correlation.

Interval Attributes: Interval attributes are numerical variables with a fixed scale and units of measurement. They represent quantities that have a clear zero point but do not have a meaningful ratio between values. Examples of interval attributes include temperature, time, or age. Interval attributes are typically used for statistical analyses and require algorithms such as linear regression, ANOVA, or t-tests.

Ratio Attributes: Ratio attributes are numerical variables with a fixed scale and units of measurement, and a meaningful zero point. They represent quantities that have a clear zero point and a meaningful ratio between values. Examples of ratio attributes include weight, height, or income. Ratio attributes are typically used for statistical analyses and require algorithms such as logistic regression or linear regression.

Processes such as derivation of other attributes, statistics, and data mining algorithms require different types of attributes depending on the specific task at hand. For example, in a classification task, nominal attributes are typically used, while in a regression task, interval or ratio attributes are preferred. Similarly, statistical analyses such as ANOVA or t-tests require interval attributes, while logistic or linear regression requires ratio attributes.

In data mining, feature engineering often involves deriving new attributes from existing ones, and the choice of attribute types used in this process depends on the specific task. For example, a derived attribute representing the ratio of income to debt requires ratio attributes, while a derived attribute representing the difference between two ordinal attributes requires ordinal attributes.













What does it mean for a statistic to be robust? Can you give examples of a robust and a non-robust statistic of the centrality of a distribution? Of its dispersion? Of the correlation between two interval-scaled attributes?



A statistic is considered robust if it is not strongly affected by outliers or extreme values in the data. In other words, the statistic remains relatively stable and accurate even when the data contains some unusual observations. A non-robust statistic, on the other hand, is strongly influenced by outliers and can lead to misleading or incorrect results.

Examples of robust and non-robust statistics for centrality of a distribution:

The median is a robust statistic for centrality because it is not affected by outliers, as it only depends on the position of the middle value. For example, the median of {1, 2, 3, 100} is still 2.5, while the mean is significantly affected and becomes 26.5.
The mean is a non-robust statistic for centrality because it is sensitive to outliers, as it takes into account the magnitude of all values. For example, the mean of {1, 2, 3, 100} is significantly affected and becomes 26.5, while the median remains 2.5.
Examples of robust and non-robust statistics for dispersion of a distribution:

The interquartile range (IQR) is a robust statistic for dispersion because it is not affected by outliers, as it only depends on the spread of the middle 50% of the values. For example, the IQR of {1, 2, 3, 100} is 2, while the standard deviation is significantly affected and becomes 46.38.
The standard deviation is a non-robust statistic for dispersion because it is sensitive to outliers, as it takes into account the magnitude of all values. For example, the standard deviation of {1, 2, 3, 100} is significantly affected and becomes 46.38, while the IQR remains 2.



Examples of robust and non-robust statistics for correlation between two interval-scaled attributes:

Spearman's rank correlation coefficient is a robust statistic for correlation because it is based on the ranks of the values rather than their magnitudes. It measures the degree to which the ranks of two variables are related, rather than the degree to which their values are related. For example, if two variables have a strong monotonic relationship, but there are some outliers, Spearman's rank correlation coefficient will still be high.
Pearson's correlation coefficient is a non-robust statistic for correlation because it is based on the actual values of the variables, and it is sensitive to outliers. It measures the degree to which two variables have a linear relationship, and it can be influenced by the magnitude of the values. For example, if two variables have a strong linear relationship, but there are some outliers, Pearson's correlation coefficient may be lower than expected.



What is an outlier? What visualization helps in discovering outliers w.r.t. one attribute?, two attributes? Do you know a way to detect outliers in a higher-dimensional space?

An outlier is an observation that is significantly different from other observations in a dataset. Outliers can be caused by measurement errors, data entry errors, or genuine extreme values.

A boxplot is a visualization technique that helps in discovering outliers with respect to one attribute. In a boxplot, the data is represented by a box that shows the interquartile range (IQR) of the data, the median, and the whiskers that extend to the minimum and maximum values that are within 1.5 times the IQR. Observations that lie outside the whiskers are considered outliers.

A scatter plot is a visualization technique that helps in discovering outliers with respect to two attributes. In a scatter plot, each observation is represented by a point that shows the values of the two attributes. Outliers can be identified as points that are far away from the other points in the plot.

In a higher-dimensional space, one way to detect outliers is to use clustering techniques. Outliers are the data points that are not assigned to any cluster or that belong to a small or sparse cluster. Another approach is to use dimensionality reduction techniques, such as principal component analysis (PCA), to project the data onto a lower-dimensional space and visualize the outliers in a scatter plot or a parallel coordinates plot. Another approach is to use anomaly detection techniques that are designed to identify outliers based on their deviation from the normal behavior of the data.


Mahalanobis Distance: This method uses the Mahalanobis distance, which is a measure of the distance between a point and the center of a distribution, taking into account the covariance matrix of the data. Outliers are defined as points that are far away from the center of the distribution, and their distance is compared to a threshold value based on a probability distribution.


